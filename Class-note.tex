\documentclass{article}
\usepackage{
    amsthm,
    amsmath,
    amssymb,
    mathtools
}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[subsection]
\newtheorem{theorem}{Theorem}[subsection]

\begin{document}
\tableofcontents
\section{Probability Theory}
\subsection{Elemantary Probability}
\begin{definition}[Cardano's Principle]
    A be a random outcome of an experiment that may proceed in various ways.
    Assume each of these ways is \textbf{equally likely}, then, probability $P[ A]$ of outcome A is
    \begin{equation}
        P[A] = \frac{number\; of\; ways\; leading\; to\; outcome\; A}{number\; of\; ways\; the\; experiment\; can\; be\; proceeded}
    \end{equation}
\end{definition}
\subsubsection{Basic principles of Counting}
\begin{itemize}
    \item Permutation:
        \begin{equation}
            A_n^k = \frac{n!}{(n-k)!}
        \end{equation}
    \item Combination:
        \begin{equation}
            \tbinom{n}{k}=\frac{n!}{(n-k)!k!}
        \end{equation}
    \item Permutation of k \textbf{Indisguishable} Objects:
        \begin{equation}
            \frac{n!}{n_1!n_2!n_3!\dotsm n_k!}=\tbinom{n}{n_1}\cdot \tbinom{n-n_1}{n_2} \dotsm \tbinom{n-(n_1+n_2 + \dotsm + n_{k-1})}{n_k}
        \end{equation}
        \emph{Remark}:\\ 
        In permutation of $k$ indistinguishable objects, the elements has noorder within a group but are differnent from each other; the groups either has order or are different from each other.\\
        \emph{Example}:\\
        Consider 10 balls, 5 red, 3 green and 2 blue. How many ways can they be arranged on a line?
        \begin{equation}
            \frac{10!}{5! \cdot 3! \cdot 2!} = 2520
        \end{equation}
\end{itemize}
\subsubsection{Sample Points, Sample Space and $\sigma$-Field }
\begin{definition}[Sample Points]
    Mathematical objects are called sample points.
\end{definition}
\begin{definition}[Sample Space]
    The sample space $S$ is large enough to accommodate all the sample points.
\end{definition}
\begin{definition}[Event]
    An outcome in the sense of Cardano's principle is interpreted as a subset $A$ of a sample space S abd called an event.
\end{definition}
\begin{definition}[Mutually exclusive]
    Two events $A_1$, $A_2$ are called mutual exclusive if $A_1\cap A_2=\emptyset$
\end{definition}
\begin{definition}[$\sigma$-Field]
    Suppose that a non-empty set $S$ is given. A \emph{$\sigma$-field} $\mathcal{F}$ on S is a family of subsets of $S$ such that:
   \begin{itemize}
       \item $\emptyset \in \mathcal{F}$
       \item If $A \in \mathcal{F}$, then $S\backslash A \in \mathcal{F}$
       \item If $A_1,A_2,\dotsm \in \mathcal{F}$ is a finit sequence of subsets, then the union $\cup_k A_k \in \mathcal{F}$
   \end{itemize}
\end{definition}
\subsubsection{Probability Measures and Spaces}
\begin{definition}[Probability Measure]
    Let $S$ be the sample space and $\mathcal{F}$ be a $\sigma$-field. Then a function
    \begin{equation}
        P:\mathcal{F} \rightarrow [0,1], A \rightarrow P[A],
    \end{equation}
    is called \emph{probability measure / probability function} on $S$ if
    \begin{itemize}
        \item $P[S]=1$
        \item For any set of events ${A_k}\subset mathcal{F}$ such that $A_j \cap A_k = \emptyset$ for $j \neq k$,
            \begin{equation}
                P[\cup_k A_k] = \sum_{k} a_n P[A_k]
            \end{equation}
    \end{itemize}
    \begin{theorem}[Basic Properties]
        $P[A_1\cup A_2] = p[A_1]+P[A_2]-P[A_1\cap A_2]$
    \end{theorem}
\end{definition}


\subsection{Conditional Probability}
\begin{definition}[Conditional Probability]
    B occurs given that A has occured
    \begin{equation}
        P[B \mid A] := \frac{P[A \cap B]}{P[A]}
    \end{equation} 
\end{definition}
\subsubsection{Independence of Events}
\begin{definition}[independent]
    Two events are \emph{independent} if
    \begin{equation}
        P[A\cap B]=P[A]P[B]
    \end{equation}
    equivalent to
    \begin{equation}
        P[A\mid B] = P[A], P[B] \neq 0
    \end{equation}
\end{definition}
\begin{definition}[Total Probability]
    \begin{equation}
        P[B] = P[B\mid A_1]*P[A_1]+\dotsm+P[B\mid A_n]P[A_n] = \sum_{k=1}^{n} P[B\mid A_k]\cdot P[A_k]
    \end{equation}
    is called total propability formula for $P[B]$
\end{definition}
\subsubsection{Bayes' Theorem}
\begin{theorem}[Bayes's Theoremm]
    Let $A_1,\dotsm, A_n \subset S$ be a set of pairwise mutually exclusive events whose union is $S$ and who each have non zero probability of occuring. Let $B\subset S$ be any events such that $P[B] \neq 0$. Then for any $A_k, k=1,\dotsm,n$
    \begin{equation}
        P[A_k\mid B] = \frac{P[B\cap A_k]}{P[B]} = \frac{P[B\mid A_k]\cdot P[A_k]}{\sum_{j=1}^{n} P[B\mid A_j]\cdot P[A_j]}
    \end{equation}
\end{theorem}

\subsection{Discrete Random Variables}
\begin{definition}[Random Variable]
    Such function X
    \begin{equation}
        X: S \rightarrow \mathbb{R}
    \end{equation}
    is a random variable. X has numerical values that are derived from the outcome of a random experiment.
\end{definition}
Two types:
\begin{itemize}
    \item \emph{Discrete Random Variables}: countable range in $\mathbb{R}$
        \begin{definition}[Discrete Random Variable]
            Let S be sample space, $\Omega$ a countable subset of $\mathbb{R}$.
            A \emph{discrete random variable} is a map
            \begin{equation}
                X: S\rightarrow \Omega
            \end{equation}
            \begin{equation}
                f_X: \Omega \rightarrow \mathbb{R}
            \end{equation}
        \end{definition}
        A random variable is often given by the pair $(X,f_X)$
    \item \emph{Continuous Random Variables}: having a range equal to $\mathbb{R}$
\end{itemize}
\emph{Example}:\\
Flip a coin three times, sample space can be given by :
\begin{equation}
    S={(t,t,t),(t,t,h),(t,h,t),\dotsm,(h,h,h)}
\end{equation}
Then we can define $X$ as follows:
\begin{equation}
    X(t,t,t)=0,X(t,t,h)=1,\dotsm,X(h,h,h)=3
\end{equation}
X denotes the number of heads
\begin{equation}
    P[X=1]=P[(t,t,h),(t,h,t),(h,t,t)]
\end{equation}
We can write
\begin{equation}
    P[X=x] = P[A]
\end{equation}
where $x \in R$ and $A \subset S$ is the event containing all sample points $p$ such that $X(p)=x$.

\subsubsection{PDF and CDF}
Random variable comes with a probability density function or probability distribution $f_X$ that allows the calculation of probability directly.\\
Follows:
\begin{itemize}
    \item $f_X(x)\textgreater 0$ for all $x$
    \item $\sum_{X\in \Omega} f_X(x)=1$ 
\end{itemize}
\begin{definition}[Cumulative distribution function]
    Cumulative distribution function of a random variable is defined as 
    \begin{equation}
        F_X:\mathbb{R}\rightarrow \mathbb{R}, F_X(x):=P[X\leq x]
    \end{equation}
    For discrete random variable, $F_X(x) = \sum_{y\leq x} f_X(y) $
\end{definition}
\subsubsection{Bernoulli Random Variable}
\begin{definition}[Bernoulli Trial]    
Consider an experiment can only results in two possible outcomes, such as success or failure.
and probability of success $0<p<1$
\end{definition}
\begin{definition}[Bernoulli Random Variable]
    Let $S$ be a sample space and $X:S\rightarrow {0,1}\in \mathbb{R}$, Let $0<P<1$, then define the density function
    \begin{equation}
        f_X:{0,1}\rightarrow R \quad f_X(x)=\left \{
            \begin{aligned}
                &\;1-p\; &for\; x=0\qquad \\
                &\;p  & for\;x=1\qquad
            \end{aligned}
    \end{equation}
    Then $X$ is said to be a \emph{Bernoulli random variable} or follow a \emph{Bernoulli distribution} with parameter $p$.
    We indicate this by writing
    \begin{equation}
        X \sim Bernoulli(p)
    \end{equation}
\end{definition}



\end{document}

